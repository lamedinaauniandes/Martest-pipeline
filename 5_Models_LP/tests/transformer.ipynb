{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61db3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamedinaa/testing_rl/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import os \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64bdc1",
   "metadata": {},
   "source": [
    "## Descarga de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b4fca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/lamedinaa/testing_rl/config.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = Path(os.getcwd()).resolve().parent\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(base_dir,'config.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7913b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lamedinaa/testing_rl/data/4_random_forest_model/datasets/abstract_episodes_4_nihal_351905032025.csv\n",
      "/home/lamedinaa/testing_rl/data/2_abstract_classes/abstract_states_4_nihal_351905032025.json\n"
     ]
    }
   ],
   "source": [
    "autor = 'nihal'\n",
    "name_file_abstract_datasets = f\"abstract_episodes_4_nihal_351905032025.csv\"\n",
    "path_abstract_datasets_dir = config.get(autor,'path_abstract_datasets')\n",
    "path_file_abstract_datasets = os.path.join(base_dir,path_abstract_datasets_dir,name_file_abstract_datasets)\n",
    "print(path_file_abstract_datasets)\n",
    "\n",
    "name_file_abstract_states = f'abstract_states_4_nihal_351905032025.json'\n",
    "path_abstract_states_dir = config.get(autor,'path_abstract_states_dir')\n",
    "path_file_abstract_states = os.path.join(base_dir,path_abstract_states_dir,name_file_abstract_states)\n",
    "print(path_file_abstract_states)\n",
    "\n",
    "### abstract to abstract class to build vocab\n",
    "f = open(path_file_abstract_states,'r')\n",
    "dict_abstract_states = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ade190",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DF DASTASET\n",
    "df_abstract_states = pd.read_csv(path_file_abstract_datasets,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3701d6",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4f2662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "## definimos el vocabularion\n",
    "especial_words = [\"1\",\"0\",\"True\",'[UNK]','[CLS]']\n",
    "vocabulary_map = {f'w{i+1}':abstract_class for i,abstract_class in enumerate(dict_abstract_states.keys()) }\n",
    "vocabulary = list(vocabulary_map.keys()) + especial_words \n",
    "## tokenizamos\n",
    "vocab =  {word:idx for idx,word in enumerate(vocabulary)}\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(vocab,unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7c21484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] [UNK] [UNK] w29 1 w29 1 w29 0 w29 0 w21 1 w21 0 w21 0 w21 1 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 1 w23 0 w23 1 w23 0 w23 1 w32 0 w32 1 w32 0 w32 0 w20 1 w20 0 w20 1 w20 0 w28 1 w28 0 w28 1 w28 0 w12 1 w12 0 w12 1 w12 0 w18 1 w18 0 w18 0 w18 1 w17 0 w17 1 w17 0 w17 1 w1 0 w1 1 w1 0 w1 1 w31 0 w31 1 w31 0 w31 1 w4 0 w4 1 w4 0 w4 0 w2 1 w2 0 w2 1 w2 0 w14 1 w14 0 w14 1 w14 0 w8 0 w8 1 w8 0 w8 1 w22 0 w22 1 w22 0 w22 0 w26 1 w26 0 w26 1 w26 0 w15 1 [UNK] 0 w15 1 w15 0 w25 0 w25 1 w25 0 w25 1 w19 0 w19 0 w19 1 w19 0 w5 1 w5 0 w5 1 w5 0 w11 0 w11 1 w11 0 w11 1 w13 0 w13 1 w13 0 w13 0 w10 1 w10 0 w10 1 w10 0 w9 0 w9 1 w9 0 w9 1 w27 0 w27 1 w27 0 w27 0 w16 1 w16 0 w16 1 w16 0 w3 1 w3 0 w3 1 w3 0 w6 0 w6 1 w6 0 w6 1 w7 0 w7 1 w7 0 True\n",
      "[CLS]w29 1 w29 1 w29 0 w29 0 w21 1 w21 0 w21 0 w21 1 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 1 w23 0 w23 1 w23 0 w23 1 w32 0 w32 1 w32 0 w32 0 w20 1 w20 0 w20 1 w20 0 w28 1 w28 0 w28 1 w28 0 w12 1 w12 0 w12 1 w12 0 w18 1 w18 0 w18 0 w18 1 w17 0 w17 1 w17 0 w17 1 w1 0 w1 1 w1 0 w1 1 w31 0 w31 1 w31 0 w31 1 w4 0 w4 1 w4 0 w4 0 w2 1 w2 0 w2 1 w2 0 w14 1 w14 0 w14 1 w14 0 w8 0 w8 1 w8 0 w8 1 w22 0 w22 1 w22 0 w22 0 w26 1 w26 0 w26 1 w26 0 w15 1 [UNK] 0 w15 1 w15 0 w25 0 w25 1 w25 0 w25 1 w19 0 w19 0 w19 1 w19 0 w5 1 w5 0 w5 1 w5 0 w11 0 w11 1 w11 0 w11 1 w13 0 w13 1 w13 0 w13 0 w10 1 w10 0 w10 1 w10 0 w9 0 w9 1 w9 0 w9 1 w27 0 w27 1 w27 0 w27 0 w16 1 w16 0 w16 1 w16 0 w3 1 w3 0 w3 1 w3 0 w6 0 w6 1 w6 0 w6 1 w7 0 w7 1 w7 0 True\n"
     ]
    }
   ],
   "source": [
    "#### get and prepare data\n",
    "df_abstract_states = pd.read_csv(path_file_abstract_datasets,sep=';')\n",
    "\n",
    "def clean_text(row):\n",
    "    return '[CLS]'+ re.sub(r'\\[\\[.*?\\]\\]', '[UNK]', row['parse_abstract_states']) + ' True'\n",
    "\n",
    "texts = df_abstract_states.apply(clean_text,axis=1)\n",
    "\n",
    "###### CODIFICACIÓN DE TEXTOS \n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer)\n",
    "fast_tokenizer.add_special_tokens({\n",
    "    'unk_token': '[UNK]',\n",
    "    'pad_token': '[PAD]'\n",
    "})\n",
    "encodings = fast_tokenizer(\n",
    "    list(texts),\n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "######## PRUEBA\n",
    "ids = [int(t) for t in list(encodings['input_ids'][0])]\n",
    "print(tokenizer.decode(ids))\n",
    "print(texts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d13d6",
   "metadata": {},
   "source": [
    "## Model Transformer txt generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aca739",
   "metadata": {},
   "source": [
    "### OPTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3105fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (49) is identical to the `bos_token_id` (49), `eos_token_id` (49), or the `sep_token_id` (None), and your input is not padded.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completada, pérdida: 0.11655746400356293\n",
      "Epoch 2 completada, pérdida: 0.05890839174389839\n",
      "Epoch 3 completada, pérdida: 0.07286675274372101\n",
      "Epoch 4 completada, pérdida: 0.054355982691049576\n",
      "Epoch 5 completada, pérdida: 0.07866627722978592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paso 1: Crear el modelo\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(fast_tokenizer),\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    bos_token_id=fast_tokenizer.pad_token_id,\n",
    "    eos_token_id=fast_tokenizer.pad_token_id,\n",
    "    pad_token_id=fast_tokenizer.pad_token_id\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "dataloader = DataLoader(encodings[\"input_ids\"], batch_size=2, shuffle=True)\n",
    "\n",
    "# Tu dataloader que da batches de input_ids\n",
    "for epoch in range(5):  # 5 épocas\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch.to(device)\n",
    "\n",
    "        outputs = model(input_ids, labels=input_ids)  # Language Modeling: entrada=salida\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completada, pérdida: {loss.item()}\")\n",
    "\n",
    "\n",
    "save_directory = '/home/lamedinaa/testing_rl/data/5_models_LP/transformers/gpt2'\n",
    "model.save_pretrained(save_directory)\n",
    "fast_tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b8e6e",
   "metadata": {},
   "source": [
    "#### OPTION 2: train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6f34e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[28, 45, 28,  ..., 49, 49, 49],\n",
       "        [34, 45, 34,  ..., 49, 49, 49],\n",
       "        [29, 45, 29,  ..., 49, 49, 49],\n",
       "        ...,\n",
       "        [23, 45, 23,  ..., 49, 49, 49],\n",
       "        [20, 45, 20,  ..., 49, 49, 49],\n",
       "        [23, 45, 23,  ..., 49, 49, 49]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff38cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.EncodingsDataset'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EncodingsDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "     Volver la clase encodings a la clase dataset necesaria para que Trainer lo acepte!!\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings[\"input_ids\"].size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "dataset = EncodingsDataset(encodings)\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a1e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 08:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1565, training_loss=0.05261854447495823, metrics={'train_runtime': 544.5562, 'train_samples_per_second': 91.744, 'train_steps_per_second': 2.874, 'total_flos': 2035551080939520.0, 'train_loss': 0.05261854447495823, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer,TrainingArguments\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_directory = '/home/lamedinaa/testing_rl/data/5_models_LP/transformers/gpt2_model3'\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(fast_tokenizer),\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    bos_token_id=fast_tokenizer.pad_token_id,\n",
    "    eos_token_id=fast_tokenizer.pad_token_id,\n",
    "    pad_token_id=fast_tokenizer.pad_token_id\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = save_directory,\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size =8, \n",
    "    save_steps=500, \n",
    "    save_total_limit=2, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args= training_args, \n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3251ce9",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33503fcb",
   "metadata": {},
   "source": [
    "##### loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95cef220",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"w29 1 w29 1 w29\"\n",
    "inputs = fast_tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w29 1 w29 1 w29 1 w21 0 w21 0 w21 1 w21 0 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 0 w23 1 w23 0 w23 1 w23 0 w32 1 w32 0 w32 1 w32 0 w20 1 w20 0 w20 1 w20 0 w28 0 w28 1 w28 0 w28 1 w12 0 w12 1 w12 0 w12 1 w18 0 w18 1 w18 0 w18 1 w17 0 w17 1 w17 0 w17 1 w1 0 w1 0 w1 1 w1 0 w31 1 w31 0 w31 1 w31 0 w4 1 w4 0 w4 1 w4 0 w2 0 w2 1 w2 0 w2 1 w14 0 w14 1 w14 0 w14 1 w8 0 w8 1 w8 0 w8 0 w22 1 w22 0 w22 1 w22 0 w26 1 w26 0 w26 0 w26 1 w15 0 w15 1 w15 0 w15 1 w25 0 w25 0 w25 1 w25 0 w19 1 w19 0 w19 1 w19 0 w5 0 w5 1 w5 0 w5 1 w11 0 w11 0 w11 1 w11 0 w13 1 w13 0 w13 1 w13 0 w10 0 w10 1 w10 0 w10 1 w9 0 w9 0 w9 1 w9 0 w27 1 w27 0 w27 1 w27 0 w16 0 w16 1 w16 0 w16 1 w3 0 w3 1 w3 0 w3 1 w6 0 w6 0 w6 1 w6 0 w7 1 w7 0 w7 1 True\n",
      "w29 1 w29 1 w29 0 w29 0 w21 1 w21 0 w21 0 w21 1 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 1 w23 0 w23 1 w23 0 w23 1 w32 0 w32 1 w32 0 w32 0 w20 1 w20 0 w20 1 w20 0 w28 1 w28 0 w28 1 w28 0 w12 1 w12 0 w12 1 w12 0 w18 1 w18 0 w18 0 w18 1 w17 0 w17 1 w17 0 w17 1 w1 0 w1 1 w1 0 w1 1 w31 0 w31 1 w31 0 w31 1 w4 0 w4 1 w4 0 w4 0 w2 1 w2 0 w2 1 w2 0 w14 1 w14 0 w14 1 w14 0 w8 0 w8 1 w8 0 w8 1 w22 0 w22 1 w22 0 w22 0 w26 1 w26 0 w26 1 w26 0 w15 1 [UNK] 0 w15 1 w15 0 w25 0 w25 1 w25 0 w25 1 w19 0 w19 0 w19 1 w19 0 w5 1 w5 0 w5 1 w5 0 w11 0 w11 1 w11 0 w11 1 w13 0 w13 1 w13 0 w13 0 w10 1 w10 0 w10 1 w10 0 w9 0 w9 1 w9 0 w9 1 w27 0 w27 1 w27 0 w27 0 w16 1 w16 0 w16 1 w16 0 w3 1 w3 0 w3 1 w3 0 w6 0 w6 1 w6 0 w6 1 w7 0 w7 1 w7 0 True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paso 3: Generar texto\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=300,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# Paso 4: Decodificar\n",
    "generated_text = fast_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c71fa5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c00efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w3 1 True 1 True 1 True 0 w34 0 w29 1 True 0 w34 1 w29 0 True 0 True 1 True\n",
      "w29 1 w29 1 w29 0 w29 0 w21 1 w21 0 w21 0 w21 1 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 1 w23 0 w23 1 w23 0 w23 1 w32 0 w32 1 w32 0 w32 0 w20 1 w20 0 w20 1 w20 0 w28 1 w28 0 w28 1 w28 0 w12 1 w12 0 w12 1 w12 0 w18 1 w18 0 w18 0 w18 1 w17 0 w17 1 w17 0 w17 1 w1 0 w1 1 w1 0 w1 1 w31 0 w31 1 w31 0 w31 1 w4 0 w4 1 w4 0 w4 0 w2 1 w2 0 w2 1 w2 0 w14 1 w14 0 w14 1 w14 0 w8 0 w8 1 w8 0 w8 1 w22 0 w22 1 w22 0 w22 0 w26 1 w26 0 w26 1 w26 0 w15 1 [UNK] 0 w15 1 w15 0 w25 0 w25 1 w25 0 w25 1 w19 0 w19 0 w19 1 w19 0 w5 1 w5 0 w5 1 w5 0 w11 0 w11 1 w11 0 w11 1 w13 0 w13 1 w13 0 w13 0 w10 1 w10 0 w10 1 w10 0 w9 0 w9 1 w9 0 w9 1 w27 0 w27 1 w27 0 w27 0 w16 1 w16 0 w16 1 w16 0 w3 1 w3 0 w3 1 w3 0 w6 0 w6 1 w6 0 w6 1 w7 0 w7 1 w7 0 True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "prompt = \"w3\"\n",
    "inputs = fast_tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=300,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,         # controla “creatividad” (1.0 = sin cambio)\n",
    "    num_return_sequences=2,\n",
    "    top_k=50,               # máximo 50\n",
    "    top_p=0.95              # elige los tokens que sumen una probabilidad >= 0.95\n",
    ")\n",
    "\n",
    "###\n",
    "generated_text = fast_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7fe72ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w29 1 w29 1 w29 0 w29 0 w21 1 w21 0 w21 1 w21 0 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 1 w23 0 w23 1 w23 0 w23 1 w32 0 w32 1 w32 0 w32 1 w20 0 w20 0 w20 1 w20 0 w28 1 w28 0 w28 1 w28 0 w12 1 w12 0 w12 1 w12 0 w18 1 w18 0 w18 0 w18 1 w17 0 w17 1 w17 0 w1 1 w1 0 w1 1 w31 0 w31 1 w31 0 w31 1 w4 0 w4 1 w4 0 w4 0 w2 1 w2 0 w2 1 w2 0 w14 1 w14 0 w14 1 w14 0 w8 0 w8 1 w8 0 w8 1 w22 0 w22 1 w22 0 w26 0 w26 1 w26 0 w26 1 w15 0 w15 1 w15 0 w25 0 w25 1 w25 0 w25 1 w19 0 w19 0 w19 1 w5 0 w5 1 w5 0 w11 1 w11 0 w11 0 w11 1 w13 0 w13 1 w13 0 w10 0 w10 1 w10 0 w9 1 w9 0 w9 0 w9 1 w27 0 w27 1 w27 0 w16 1 w16 0 w16 0 w3 1 w3 0 w3 1 w3 0 w6 1 w6 0 w6 0 w7 1 w7 0 True\n",
      "w29 1 w29 1 w29 0 w29 0 w21 1 w21 0 w21 0 w21 1 w24 0 w24 1 w24 0 w24 1 w30 0 w30 1 w30 0 w30 1 w23 0 w23 1 w23 0 w23 1 w32 0 w32 1 w32 0 w32 0 w20 1 w20 0 w20 1 w20 0 w28 1 w28 0 w28 1 w28 0 w12 1 w12 0 w12 1 w12 0 w18 1 w18 0 w18 0 w18 1 w17 0 w17 1 w17 0 w17 1 w1 0 w1 1 w1 0 w1 1 w31 0 w31 1 w31 0 w31 1 w4 0 w4 1 w4 0 w4 0 w2 1 w2 0 w2 1 w2 0 w14 1 w14 0 w14 1 w14 0 w8 0 w8 1 w8 0 w8 1 w22 0 w22 1 w22 0 w22 0 w26 1 w26 0 w26 1 w26 0 w15 1 [UNK] 0 w15 1 w15 0 w25 0 w25 1 w25 0 w25 1 w19 0 w19 0 w19 1 w19 0 w5 1 w5 0 w5 1 w5 0 w11 0 w11 1 w11 0 w11 1 w13 0 w13 1 w13 0 w13 0 w10 1 w10 0 w10 1 w10 0 w9 0 w9 1 w9 0 w9 1 w27 0 w27 1 w27 0 w27 0 w16 1 w16 0 w16 1 w16 0 w3 1 w3 0 w3 1 w3 0 w6 0 w6 1 w6 0 w6 1 w7 0 w7 1 w7 0 True\n"
     ]
    }
   ],
   "source": [
    "generated_text = fast_tokenizer.decode(outputs[1], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09d58331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0135,  0.0093, -0.0225,  ...,  0.0290, -0.0041, -0.0025],\n",
       "        [-0.0215,  0.0151,  0.0091,  ...,  0.0241, -0.0054,  0.0267],\n",
       "        [ 0.0066, -0.0003, -0.0704,  ...,  0.0288,  0.0062,  0.0195],\n",
       "        ...,\n",
       "        [ 0.0125, -0.0450, -0.0536,  ...,  0.0143,  0.0027, -0.0330],\n",
       "        [-0.0076,  0.0061, -0.0181,  ...,  0.0318, -0.0044,  0.0029],\n",
       "        [ 0.0103,  0.0113,  0.0043,  ..., -0.0055,  0.0324,  0.0190]],\n",
       "       device='cuda:3', requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.transformer.wte.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1950d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.modules.module.Module.__dir__(self)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2LMHeadModel.__dir__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f2a134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_assisted_decoding',\n",
       " '_auto_class',\n",
       " '_autoset_attn_implementation',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_beam_search',\n",
       " '_beam_search_has_unfinished_sequences',\n",
       " '_cache_dependant_input_preparation',\n",
       " '_cache_dependant_input_preparation_exporting',\n",
       " '_call_impl',\n",
       " '_check_and_enable_flash_attn_2',\n",
       " '_check_and_enable_flex_attn',\n",
       " '_check_and_enable_sdpa',\n",
       " '_compiled_call_impl',\n",
       " '_constrained_beam_search',\n",
       " '_contrastive_search',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_dola_decoding',\n",
       " '_expand_inputs_for_generation',\n",
       " '_fix_state_dict_key_on_load',\n",
       " '_fix_state_dict_key_on_save',\n",
       " '_fix_state_dict_keys_on_save',\n",
       " '_flatten_beam_dim',\n",
       " '_from_config',\n",
       " '_gather_beams',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_cache',\n",
       " '_get_candidate_generator',\n",
       " '_get_files_timestamps',\n",
       " '_get_initial_cache_position',\n",
       " '_get_key_renaming_mapping',\n",
       " '_get_layer_device_map_for_cache_init',\n",
       " '_get_logits_processor',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_running_beams_for_next_iteration',\n",
       " '_get_stopping_criteria',\n",
       " '_get_top_k_continuations',\n",
       " '_group_beam_search',\n",
       " '_has_unfinished_sequences',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_added_embeddings_weights_with_mean',\n",
       " '_init_added_lm_head_bias_with_mean',\n",
       " '_init_added_lm_head_weights_with_mean',\n",
       " '_init_weights',\n",
       " '_initialize_missing_keys',\n",
       " '_initialize_weights',\n",
       " '_is_stateful',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_flax',\n",
       " '_load_from_state_dict',\n",
       " '_load_from_tf',\n",
       " '_load_pretrained_model',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_move_missing_keys_from_meta_to_cpu',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_pp_plan',\n",
       " '_prefill_chunking',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_cache_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_generated_length',\n",
       " '_prepare_generation_config',\n",
       " '_prepare_model_inputs',\n",
       " '_prepare_special_tokens',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_sample',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_supports_attention_backend',\n",
       " '_supports_cache_class',\n",
       " '_supports_default_dynamic_cache',\n",
       " '_supports_flash_attn_2',\n",
       " '_supports_flex_attn',\n",
       " '_supports_logits_to_keep',\n",
       " '_supports_quantized_cache',\n",
       " '_supports_sdpa',\n",
       " '_supports_static_cache',\n",
       " '_temporary_reorder_cache',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_tp_plan',\n",
       " '_unflatten_beam_dim',\n",
       " '_update_finished_beams',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_assistant',\n",
       " '_validate_generated_length',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'active_adapter',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'compute_transition_scores',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'delete_adapter',\n",
       " 'deparallelize',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_compiled_call',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_init_context',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_parameter_or_buffer',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'heal_tokens',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_backend_compatible',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_parallelizable',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'loss_function',\n",
       " 'main_input_name',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parallelize',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'supports_pp_plan',\n",
       " 'supports_tp_plan',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(GPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85f9851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_call_one',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_upload_modified_files',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fast_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
